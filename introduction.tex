\section{Introduction}
\label{sec:introduction}
This seminar paper deals with the estimation of large-scale Bayesian vector autoregressive (BVAR) models. It presents key insights of the paper \citetitle{hauzenberger_combining_2021} by \textcite{hauzenberger_combining_2021}, focusing on achieving sparsity in a shrunk BVAR model and validating the approach in a practical forecasting application.

While the estimation of VARs has traditionally been conducted using ordinary-least-squares (OLS) the Bayesian literature addresses the common overfitting issue of OLS with informative priors. Bayesian inference relies on combining prior knowledge about model quantities with evidence found in the data to obtain full posterior distributions. In large dimensional models such as VAR models, shrinkage priors are a helpful tool to control for overfitting to the training set. In this work, the common Minnesota shrinkage prior invented by \textcite{litterman_forecasting_1986} is used to shrink the VAR model coefficients in order to avoid overfitting. An important property of the Minnesota prior is that it can lead to a natural conjugate model so that important distributions, such as the marginal posterior distributions and the predictive density, are available in closed form and do not need to be approximated. This approach stands in contrast to non-conjugate models like the BVAR with stochastic-search-variable-selection (SSVS) prior \parencite{george_bayesian_2008}. The main advantage of conjugate models in comparison to the non-conjugate competitor is computational speed because the posterior distributions in non-conjugate models need to be simulated using methods like Markov-Chain-Monte-Carlo (MCMC). At the same time, the non-conjugate SSVS prior model allows for more flexibility in regards to the prior restrictions the researcher can impose on the model parameters \parencite{banbura_large_2010,koop_bayesian_2009,koop_forecasting_2013,george_bayesian_2008}.

\textcite{hauzenberger_combining_2021} argue that shrinkage alone is not sufficient in conjugate models as the probability of setting parameters to exactly zero in the Minnesota prior setup is zero. To remove this upper bound of accuracy, they propose the post-processing of point-estimates to obtain a sparse model.

The remainder of this seminar paper is structured as follows: Section~\ref{sec:theoretical_background} introduces the topic of shrinkage and sparsity illustrated by the instance of the Bayesian lasso. In addition, a general definition of VAR models is given. Subsequently, Section~\ref{sec:combining_shrinkage_and_sparsity} presents the key approach of \textcite{hauzenberger_combining_2021} towards combining shrinkage and sparsity in BVAR models. The theoretical approach presented in Section~\ref{sec:combining_shrinkage_and_sparsity} is then validated in Section~\ref{sec:forecasting_application} by the example of a macroeconomic forecasting application using the \textcite{mccracken_fred-md_2015} dataset. Finally, the paper concludes in Section~\ref{sec:conclusion}.
